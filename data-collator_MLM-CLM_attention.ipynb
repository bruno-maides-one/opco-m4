{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test avec un MLM",
   "id": "b70b4cdfb3afe895"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T17:25:17.552880Z",
     "start_time": "2025-08-01T17:25:16.174533Z"
    }
   },
   "source": [
    "# Test avec un MLM\n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "# Charger un tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Cr√©er le data collator pour le Masked Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Activer le masquage de mots\n",
    "    mlm_probability=0.15  # Probabilit√© de masquer un jeton\n",
    ")\n",
    "\n",
    "# Exemple de phrases tokenis√©es\n",
    "tokenized_sentences = [\n",
    "    {\"input_ids\": [101, 7592, 1010, 2023, 2003, 102]},\n",
    "    {\"input_ids\": [101, 2023, 2003, 1037, 2503, 102]}\n",
    "]\n",
    "\n",
    "# Appliquer le data collator\n",
    "batch = data_collator(tokenized_sentences)\n",
    "\n",
    "# Le 'batch' contiendra les 'input_ids' avec certains jetons masqu√©s\n",
    "# et les 'labels' correspondants pour l'entra√Ænement.\n",
    "print(batch)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  103, 1010, 2023, 2003,  102],\n",
      "        [ 101, 2023, 2003, 1037,  103,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, 7592, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, 2503, -100]])}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dans les MLM on utilise un masquage d'un ou plusieurs tokens comme label, c'est repr√©sent√© par les labels\n",
    "\n",
    "d√©finition : label = bonne r√©ponse dans une entr√©e\n",
    "\n",
    "Exemple de sortie du code ci dessus avec 1 mask\n",
    "```\n",
    "{\n",
    "'input_ids': tensor([\n",
    "    [ 101, 7592, 1010, 2023, 2003,  102],\n",
    "    [ 101, 2023,  103, 1037, 2503,  102]\n",
    "]),\n",
    "'attention_mask': tensor([\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]),\n",
    "'labels': tensor([\n",
    "    [-100, -100, -100, -100, -100, -100],\n",
    "    [-100, -100, 2003, -100, -100, -100]\n",
    "])\n",
    "}\n",
    "```\n",
    "\n",
    "Exemple avec 2 masks\n",
    "```\n",
    "{\n",
    "'input_ids': tensor([\n",
    "    [ 101,  103, 1010, 2023, 2003,  102],\n",
    "    [ 101, 2023, 2003, 1037,  103,  102]\n",
    "]),\n",
    "'attention_mask': tensor([\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "]),\n",
    "'labels': tensor([\n",
    "    [-100, 7592, -100, -100, -100, -100],\n",
    "    [-100, -100, -100, -100, 2503, -100]\n",
    "])\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "principe g√©n√©ral :\n",
    "\n",
    "| s√©quence      | 0    | 1    | 2    | 3      | 4        | 5 |\n",
    "|---------------|------|------|------|--------|----------|---|\n",
    "| **entr√©e**    | La   | chat | a    | quatre | papattes | ! |\n",
    "| **input_ids** | La   | chat | a    | 103    | papattes | ! |\n",
    "| **labels**    | -100 | -100 | -100 | quatre | -100     | ! |\n",
    "\n",
    "Ici le 103 dans input_ids correspond au token MASK, c'est a dire la valeur a masquer.\n",
    "\n",
    "Le -100 dans label repr√©sente que la valeur ne va pas √™tre test√©e.\n",
    "\n",
    "Ce qu'il se passe c'est que le model essais de pr√©dire le mot masque et calcul la diff√©rence entre la prediction et la bonne r√©ponse pour d√©termin√© la grandeur de l'erreur.\n"
   ],
   "id": "c1f0b63e7f53fa1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test avec le GML",
   "id": "951dc3deb78e7e7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T17:36:31.277657Z",
     "start_time": "2025-08-01T17:36:30.495829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# 1. Charger le tokenizer sp√©cifique au mod√®le CodeGen\n",
    "#    Ce tokenizer est optimis√© pour du code Python.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "\n",
    "# Si le tokenizer n'a pas de jeton de padding, on lui en assigne un.\n",
    "# Le jeton EOS (End Of Sentence) est souvent un bon candidat.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Cr√©er le Data Collator pour le Causal Language Modeling\n",
    "#    La seule diff√©rence est mlm=False.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # üëà C'est le changement crucial !\n",
    ")\n",
    "\n",
    "# 3. Exemple avec des extraits de code Python\n",
    "python_code_snippets = [\n",
    "    \"def hello_world():\",\n",
    "    \"import numpy as np\"\n",
    "]\n",
    "\n",
    "# Tokeniser les extraits de code\n",
    "tokenized_code = tokenizer(python_code_snippets, truncation=True)\n",
    "\n",
    "# 4. Appliquer le data collator pour cr√©er un lot\n",
    "batch = data_collator([{\"input_ids\": item} for item in tokenized_code[\"input_ids\"]])\n",
    "\n",
    "# 5. Analyser le r√©sultat\n",
    "print(\"--- Input IDs ---\")\n",
    "print(batch['input_ids'])\n",
    "\n",
    "print(\"\\n--- Labels ---\")\n",
    "print(batch['labels'])\n",
    "\n",
    "batch"
   ],
   "id": "5bfe671a862502ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input IDs ---\n",
      "tensor([[ 4299, 23748,    62,  6894, 33529],\n",
      "        [11748,   299, 32152,   355, 45941]])\n",
      "\n",
      "--- Labels ---\n",
      "tensor([[ 4299, 23748,    62,  6894, 33529],\n",
      "        [11748,   299, 32152,   355, 45941]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4299, 23748,    62,  6894, 33529],\n",
       "        [11748,   299, 32152,   355, 45941]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]]), 'labels': tensor([[ 4299, 23748,    62,  6894, 33529],\n",
       "        [11748,   299, 32152,   355, 45941]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Le principe est fondamentalement diff√©rent, le CML va pouvoir pr√©dire le dernier mot de la s√©quence proposer en entr√©.\n",
    "\n",
    "Dans la sequence [ 4299, 23748, 62, 6894, 33529] le model cherchera √† deviner 33529 √† partir des token [ 4299, 23748, 62, 6894]\n",
    "\n",
    "le masque d'attention est interne au model et se nome masque d'attention causal\n"
   ],
   "id": "a76d64075d7884c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "\n",
    "* [ ] Approfondir le masque d'attention causal, entre autre comment se d√©roule la pr√©dition du token suivant la s√©quence actuellement active.\n",
    "* [ ] Role du padding mask dans le CML, c'est pas compl√©tement clair.\n",
    "* [ ] V√©rifier le m√©canisme de cross attention."
   ],
   "id": "5f4f6dcb8837c206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sources\n",
    "\n",
    "## Diff√©rence entre MLM et CLM\n",
    "\n",
    "https://medium.com/data-science/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5\n",
    "\n",
    "https://www.youtube.com/watch?v=agRyKPhsqJ4\n",
    "\n",
    "## Cross-attention\n",
    "\n",
    "https://datascience.stackexchange.com/questions/126187/cross-attention-mask-in-transformers\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n"
   ],
   "id": "d1a06d45c3301bc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28cf94e7c9c2c7d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
