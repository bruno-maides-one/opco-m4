{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:26:21.753406Z",
     "start_time": "2025-07-29T12:26:21.750165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Utilisation du pÃ©riphÃ©rique : {device}\")\n",
    "torch.cuda.get_device_name()\n"
   ],
   "id": "fd1c1ad9a77566cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du pÃ©riphÃ©rique : cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Laptop GPU'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chargement du dataset",
   "id": "1cf043f039b4e6f6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-29T12:26:28.346913Z",
     "start_time": "2025-07-29T12:26:24.986112Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"bigcode/the-stack-smol-xl\", data_dir=\"data/dockerfile\")\n",
    "dataset = load_dataset(\"bigcode/the-stack-smol-xl\", data_dir=\"data/python\", split=\"train[:20]\")\n",
    "\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hexsha', 'size', 'ext', 'lang', 'max_stars_repo_path', 'max_stars_repo_name', 'max_stars_repo_head_hexsha', 'max_stars_repo_licenses', 'max_stars_count', 'max_stars_repo_stars_event_min_datetime', 'max_stars_repo_stars_event_max_datetime', 'max_issues_repo_path', 'max_issues_repo_name', 'max_issues_repo_head_hexsha', 'max_issues_repo_licenses', 'max_issues_count', 'max_issues_repo_issues_event_min_datetime', 'max_issues_repo_issues_event_max_datetime', 'max_forks_repo_path', 'max_forks_repo_name', 'max_forks_repo_head_hexsha', 'max_forks_repo_licenses', 'max_forks_count', 'max_forks_repo_forks_event_min_datetime', 'max_forks_repo_forks_event_max_datetime', 'content', 'avg_line_length', 'max_line_length', 'alphanum_fraction'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chargement du model\n",
   "id": "df24aa44f4249187"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:26:35.191472Z",
     "start_time": "2025-07-29T12:26:32.511277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from opco_4_utils import init_model, do_predict\n",
    "\n",
    "tokenizer, model = init_model(\"Salesforce/codegen-350M-mono\")\n",
    "\n",
    "\n"
   ],
   "id": "1ad1bb6ddbfef2ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:26:42.991666Z",
     "start_time": "2025-07-29T12:26:42.984586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On ne conserve que le code car rien d'autre interessant dans le dataset\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda ex: {\n",
    "        'code': ex['content']\n",
    "    },\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "dataset"
   ],
   "id": "f669dcf1aabcaf5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Le dataset est prÃ©parer\n",
    "\n",
    "il faut maintenant 'tokeniser' les lignes"
   ],
   "id": "b59b17190ad1b1b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:26:47.460117Z",
     "start_time": "2025-07-29T12:26:47.439920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenised_dataset = dataset.map(\n",
    "    lambda ex: tokenizer(ex['code'])\n",
    ")\n",
    "\n",
    "tokenised_dataset"
   ],
   "id": "bc5ea1abf6c8b9fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:29:21.597106Z",
     "start_time": "2025-07-29T12:29:21.580523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# DÃ©finir le rÃ©pertoire de sortie pour le modÃ¨le fine-tunÃ©\n",
    "output_dir = os.path.join(\"export_model\", \"./codegen-350m-mono-finetuned-python\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Un seul epoch peut dÃ©jÃ  donner de bons rÃ©sultats\n",
    "    per_device_train_batch_size=4,  # RÃ©duire si vous avez des erreurs de mÃ©moire (OOM)\n",
    "    gradient_accumulation_steps=2, # Simule un batch size plus grand (4*2=8)\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,  # Utiliser le mixed precision pour accÃ©lÃ©rer l'entraÃ®nement (nÃ©cessite un GPU compatible)\n",
    "    logging_steps=500,\n",
    "#    report_to=\"tensorboard\", # Optionnel: pour visualiser les logs\n",
    ")\n"
   ],
   "id": "6b771f98323faacc",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T12:29:25.627526Z",
     "start_time": "2025-07-29T12:29:24.624953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Le Data Collator prÃ©pare les batches pour le language modeling\n",
    "# mlm=False car c'est du Causal LM (style GPT), pas du Masked LM (style BERT)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Initialiser le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Lancer le fine-tuning ðŸš€\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le modÃ¨le final\n",
    "trainer.save_model(output_dir)\n",
    "print(f\"ModÃ¨le sauvegardÃ© dans {output_dir}\")"
   ],
   "id": "5ef9ed7ff1b54ce2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27131/411985426.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m      6\u001B[39m trainer = Trainer(\n\u001B[32m      7\u001B[39m     model=model,\n\u001B[32m      8\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     11\u001B[39m     tokenizer=tokenizer,\n\u001B[32m     12\u001B[39m )\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Lancer le fine-tuning ðŸš€\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m trainer.train()\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# Sauvegarder le modÃ¨le final\u001B[39;00m\n\u001B[32m     18\u001B[39m trainer.save_model(output_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/trainer.py:2237\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2235\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2236\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2237\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[32m   2238\u001B[39m         args=args,\n\u001B[32m   2239\u001B[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001B[32m   2240\u001B[39m         trial=trial,\n\u001B[32m   2241\u001B[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001B[32m   2242\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/trainer.py:2532\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2530\u001B[39m update_step += \u001B[32m1\u001B[39m\n\u001B[32m   2531\u001B[39m num_batches = args.gradient_accumulation_steps \u001B[38;5;28;01mif\u001B[39;00m update_step != (total_updates - \u001B[32m1\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m remainder\n\u001B[32m-> \u001B[39m\u001B[32m2532\u001B[39m batch_samples, num_items_in_batch = \u001B[38;5;28mself\u001B[39m.get_batch_samples(epoch_iterator, num_batches, args.device)\n\u001B[32m   2533\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch_samples):\n\u001B[32m   2534\u001B[39m     step += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/trainer.py:5343\u001B[39m, in \u001B[36mTrainer.get_batch_samples\u001B[39m\u001B[34m(self, epoch_iterator, num_batches, device)\u001B[39m\n\u001B[32m   5341\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_batches):\n\u001B[32m   5342\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m5343\u001B[39m         batch_samples.append(\u001B[38;5;28mnext\u001B[39m(epoch_iterator))\n\u001B[32m   5344\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m   5345\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/accelerate/data_loader.py:567\u001B[39m, in \u001B[36mDataLoaderShard.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    565\u001B[39m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[32m    566\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m567\u001B[39m     current_batch = \u001B[38;5;28mnext\u001B[39m(dataloader_iter)\n\u001B[32m    568\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    569\u001B[39m     \u001B[38;5;28mself\u001B[39m.end()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28mself\u001B[39m._dataset_fetcher.fetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.collate_fn(data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/data/data_collator.py:46\u001B[39m, in \u001B[36mDataCollatorMixin.__call__\u001B[39m\u001B[34m(self, features, return_tensors)\u001B[39m\n\u001B[32m     44\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tf_call(features)\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m return_tensors == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.torch_call(features)\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m return_tensors == \u001B[33m\"\u001B[39m\u001B[33mnp\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     48\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.numpy_call(features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/data/data_collator.py:1014\u001B[39m, in \u001B[36mDataCollatorForLanguageModeling.torch_call\u001B[39m\u001B[34m(self, examples)\u001B[39m\n\u001B[32m   1011\u001B[39m     \u001B[38;5;28mself\u001B[39m.create_rng()\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(examples[\u001B[32m0\u001B[39m], Mapping):\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     batch = pad_without_fast_tokenizer_warning(\n\u001B[32m   1015\u001B[39m         \u001B[38;5;28mself\u001B[39m.tokenizer, examples, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m, pad_to_multiple_of=\u001B[38;5;28mself\u001B[39m.pad_to_multiple_of\n\u001B[32m   1016\u001B[39m     )\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1018\u001B[39m     batch = {\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m: _torch_collate_batch(examples, \u001B[38;5;28mself\u001B[39m.tokenizer, pad_to_multiple_of=\u001B[38;5;28mself\u001B[39m.pad_to_multiple_of)\n\u001B[32m   1020\u001B[39m     }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/data/data_collator.py:67\u001B[39m, in \u001B[36mpad_without_fast_tokenizer_warning\u001B[39m\u001B[34m(tokenizer, *pad_args, **pad_kwargs)\u001B[39m\n\u001B[32m     64\u001B[39m tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     69\u001B[39m     \u001B[38;5;66;03m# Restore the state of the warning.\u001B[39;00m\n\u001B[32m     70\u001B[39m     tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = warning_state\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3331\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.pad\u001B[39m\u001B[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001B[39m\n\u001B[32m   3328\u001B[39m         encoded_inputs[key] = to_py_obj(value)\n\u001B[32m   3330\u001B[39m \u001B[38;5;66;03m# Convert padding_strategy in PaddingStrategy\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3331\u001B[39m padding_strategy, _, max_length, _ = \u001B[38;5;28mself\u001B[39m._get_padding_truncation_strategies(\n\u001B[32m   3332\u001B[39m     padding=padding, max_length=max_length, verbose=verbose\n\u001B[32m   3333\u001B[39m )\n\u001B[32m   3335\u001B[39m required_input = encoded_inputs[\u001B[38;5;28mself\u001B[39m.model_input_names[\u001B[32m0\u001B[39m]]\n\u001B[32m   3336\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m required_input \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(required_input[\u001B[32m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/opco-module-4/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2750\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001B[39m\u001B[34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001B[39m\n\u001B[32m   2748\u001B[39m \u001B[38;5;66;03m# Test if we have a padding token\u001B[39;00m\n\u001B[32m   2749\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.pad_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pad_token_id < \u001B[32m0\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m2750\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   2751\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAsking to pad but the tokenizer does not have a padding token. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2752\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2753\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mor add a new pad token via `tokenizer.add_special_tokens(\u001B[39m\u001B[33m{\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpad_token\u001B[39m\u001B[33m'\u001B[39m\u001B[33m: \u001B[39m\u001B[33m'\u001B[39m\u001B[33m[PAD]\u001B[39m\u001B[33m'\u001B[39m\u001B[33m})`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2754\u001B[39m     )\n\u001B[32m   2756\u001B[39m \u001B[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001B[39;00m\n\u001B[32m   2757\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2758\u001B[39m     truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n\u001B[32m   2759\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD\n\u001B[32m   (...)\u001B[39m\u001B[32m   2762\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (max_length % pad_to_multiple_of != \u001B[32m0\u001B[39m)\n\u001B[32m   2763\u001B[39m ):\n",
      "\u001B[31mValueError\u001B[39m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3669baf573909ade"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
